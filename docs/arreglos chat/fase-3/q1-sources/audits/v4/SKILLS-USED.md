# SKILLS-USED.md — Anti-Hallucination Audit v4

**Date:** 2026-02-18
**Auditor:** Claude Code (Opus 4.6)

---

## Skills Invoked

| # | Skill Requested | Status | What It Provided | Fallback |
|---|----------------|--------|------------------|----------|
| 1 | `skill-orchestrator` | LOADED (NOT APPLICABLE) | Marketing/business workflow orchestrator — no code audit capability | Used manual audit orchestration via 6 parallel agents |
| 2 | `code-auditor` | LOADED (APPLICABLE) | Code audit methodology: architecture, quality, security, performance, testing, maintainability | Used directly: phantom detection, drift analysis, claims extraction |
| 3 | `test-master` | LOADED (APPLICABLE) | Testing specialist: pytest collect-only, pytest -v, coverage analysis, test quality | Used directly: `pytest --collect-only` + `pytest -v` + ruff lint |
| 4 | `systematic-debugging` | LOADED (APPLICABLE) | 4-phase debugging: root cause investigation, pattern analysis, hypothesis testing, implementation | Used for: explaining "0 tests" discrepancy in v3 ground-truth-counts.txt |
| 5 | `rag-architect` | LOADED (APPLICABLE) | RAG systems expertise: vector DB, embeddings, chunking, retrieval, evaluation | Used for: semantic inflation analysis of RAG/retrieval claims in docs |
| 6 | `dispatching-parallel-agents` | LOADED (APPLICABLE) | Parallel agent dispatch pattern for independent problem domains | Used directly: 6 agents dispatched in parallel (A1-A6) |

---

## Skill Usage Details

### skill-orchestrator
- **Invoked:** Yes
- **Applicable:** No — designed for marketing/product launch workflows, not code forensics
- **Fallback:** Manual orchestration using `dispatching-parallel-agents` pattern
- **Documented:** This file serves as the orchestration record

### code-auditor
- **Invoked:** Yes
- **Applied to:** A1 (Claim Extraction), A2 (Evidence Verification), A4 (Schema Deep Verify), A6 (Meta-Consistency)
- **Approach followed:** Explore → Identify patterns → Read critical files → Run analysis → Synthesize
- **Output format:** Claims JSONL + evidence files + audit report

### test-master
- **Invoked:** Yes
- **Applied to:** A3 (Tests & Tooling Verifier)
- **Commands executed:**
  - `pytest --collect-only` → 5 tests collected (evidence/pytest-collect-only.txt)
  - `pytest tests/unit/test_validators.py -v` → 5/5 PASS (evidence/pytest-run.txt)
  - `ruff check` → All checks passed (evidence/ruff.txt)
- **Constraint verified:** "MUST DO: Test happy paths AND error cases" — test_validators.py has 3 positive + 2 negative tests

### systematic-debugging
- **Invoked:** Yes
- **Applied to:** Discrepancy investigation
- **Root cause found:** v3 `ground-truth-counts.txt` reported "test_validators.py: 0 tests" because the counting script used `grep -c 'def test_'` on a file path that didn't resolve correctly. Actual pytest `--collect-only` shows 5 items.
- **Phase 1 evidence:** pytest-collect-only.txt confirms 5 tests exist

### rag-architect
- **Invoked:** Yes
- **Applied to:** A5 (Semantic Inflation Red Team)
- **Constraint applied:** "MUST NOT: Skip metadata enrichment" — checked that claims about RAG readiness distinguish between research-documented URLs vs HTTP-verified vs schema-validated
- **Finding:** Documentation uses "verified" 80+ times but only schema-validation has machine evidence. RAG retrieval claims (Q2/Q3 scope) are correctly scoped as backlog items, not current capabilities.

### dispatching-parallel-agents
- **Invoked:** Yes
- **Applied to:** Full audit orchestration
- **Pattern followed:** 6 independent domains (A1-A6), each with focused scope, clear goal, specific output
- **Agents dispatched:** A1 Claim Extractor, A2 Evidence Verifier, A3 Tests Verifier, A4 Schema Deep Verifier, A5 Semantic Red Team, A6 Meta-Consistency Checker

---

*Generated by v4 audit, 2026-02-18*

{"sf_id": "SF-001", "file": "docs/arreglos chat/fase-3/q1-sources/Q1.1-BIBLIOTECA-OFICIAL-v0-REPORT.md", "line": 11, "original": "The Biblioteca Oficial v0 delivers a fully validated, machine-readable foundation for Clara's knowledge base.", "problem": "\"fully validated\" implies HTTP reachability, content correctness, and completeness validation. In reality, only JSON Schema structural validation was performed. No URLs were HTTP-checked (Gap G1 in assumptions-gaps.md), no content was verified for accuracy, and the forensic audit found the link checker crashes at URL 12/18. \"fully validated\" is semantic inflation of \"schema-validated\".", "severity": "CRITICAL", "replacement": "The Biblioteca Oficial v0 delivers a schema-validated, machine-readable foundation for Clara's knowledge base. (Note: HTTP validation of URLs and content verification are deferred to Q2.)"}
{"sf_id": "SF-002", "file": "docs/arreglos chat/fase-3/q1-sources/Q1.1-BIBLIOTECA-OFICIAL-v0-REPORT.md", "line": 11, "original": "All Q1 research (AGE, CCAA, municipal sources) has been converted into structured YAML registries, JSON schemas, and governance policies with automated validation gates.", "problem": "\"All Q1 research\" implies total conversion. The 9 research markdown files (4,448 lines of detailed prose) were not converted -- they remain as markdown. What was converted is the structured data (source lists, URLs, rules) into YAML/JSON. The research analysis, rationale, edge cases, and design decisions in the .md files were not converted to machine-readable form.", "severity": "HIGH", "replacement": "The structured data from Q1 research (source registries, governance rules, schema definitions) has been converted into YAML registries, JSON schemas, and governance policies with automated validation gates. (The 9 research markdown files remain as human-readable documentation.)"}
{"sf_id": "SF-003", "file": "docs/arreglos chat/fase-3/q1-sources/Q1.1-BIBLIOTECA-OFICIAL-v0-REPORT.md", "line": 15, "original": "44 government sources cataloged in `registry.yaml` (25 AGE + 19 CCAA), validated against JSON Schema", "problem": "\"validated against JSON Schema\" is true for structural format, but a reader could infer that the 44 source URLs themselves were validated as reachable/correct. The forensic audit found: 2 BOE API URLs are stubs (404/400), 2 AGE URLs return 403 bot-protection, 14 allowlist coverage gaps exist. Schema validation checks field types and required properties, not URL liveness or content accuracy.", "severity": "HIGH", "replacement": "44 government sources cataloged in `registry.yaml` (25 AGE + 19 CCAA), structurally validated against JSON Schema (URL liveness not verified -- deferred to Q2)"}
{"sf_id": "SF-004", "file": "docs/arreglos chat/fase-3/q1-sources/Q1.1-BIBLIOTECA-OFICIAL-v0-REPORT.md", "line": 19, "original": "4 validation scripts (`validate_source_registry.py`, `validate_policy.py`, `validate_proceduredoc_schema.py`, `link_check.py`) — all executable, all passing", "problem": "\"all passing\" is misleading for link_check.py. The forensic audit (F-01) found that link_check.py crashes at URL 12/18 during live smoke test due to a NoneType bug on line ~111. While the fix was reportedly applied (AUDIT-01), the original Q1.1 report still states \"all passing\" without caveat. Even post-fix, link_check.py only tested 3 URLs in smoke mode -- it did not pass a comprehensive validation.", "severity": "CRITICAL", "replacement": "4 validation scripts (`validate_source_registry.py`, `validate_policy.py`, `validate_proceduredoc_schema.py`, `link_check.py`) — all executable; 3 schema validators pass; link_check.py passes in dry-run/smoke mode (3 URLs tested, live full-scan not performed)"}
{"sf_id": "SF-005", "file": "docs/arreglos chat/fase-3/q1-sources/Q1.1-BIBLIOTECA-OFICIAL-v0-REPORT.md", "line": 21, "original": "3 unit tests covering all validators pass in CI-compatible pytest", "problem": "\"covering all validators\" is misleading. The forensic audit (F-04) found that all 3 tests only verify happy paths -- no negative test cases. Even after AUDIT-04 added 2 negative tests (total 5), the Q1.1 report still says \"3 unit tests\" and \"covering all validators.\" The tests cover 3 of 4 validators (link_check.py has no unit test). \"All\" is false; coverage is partial.", "severity": "HIGH", "replacement": "3 unit tests covering schema validators (happy-path only) pass in CI-compatible pytest. (Note: link_check.py has no unit test; negative cases added post-audit bring total to 5 tests.)"}
{"sf_id": "SF-006", "file": "docs/arreglos chat/fase-3/q1-sources/Q1.1-BIBLIOTECA-OFICIAL-v0-REPORT.md", "line": 22, "original": "All 6 gates passed (G1-G6), all 4 abort conditions cleared (A1-A4)", "problem": "\"All 6 gates passed\" conceals that G4 (link checker smoke test) was MISLEADING per the forensic audit -- the live test crashed at URL 12/18. Only dry-run mode worked. The forensic audit explicitly marks G4 as \"PARTIAL\" / \"MISLEADING\" yet this summary bullet claims unqualified PASS for all 6.", "severity": "CRITICAL", "replacement": "5 of 6 gates passed cleanly (G1-G3, G5-G6); G4 (link checker) passed in dry-run mode only (live test had a crash bug, since fixed). All 4 abort conditions cleared (A1-A4)."}
{"sf_id": "SF-007", "file": "docs/arreglos chat/fase-3/q1-sources/Q1.1-BIBLIOTECA-OFICIAL-v0-REPORT.md", "line": 20, "original": "Link checker supports smoke mode, dry-run, JSONL output, per-domain rate limiting — live test: 3/3 URLs OK (avg 229ms)", "problem": "\"3/3 URLs OK\" uses denominator concealment. The registry contains 64 sources with 100+ URLs total. Testing 3 URLs and reporting \"3/3\" creates a false impression of comprehensive validation. The forensic audit showed only 7/11 URLs truly returned OK before the script crashed at URL 12/18. The denominator 3 is cherry-picked from a limited smoke test.", "severity": "CRITICAL", "replacement": "Link checker supports smoke mode, dry-run, JSONL output, per-domain rate limiting — smoke test: 3/3 sampled URLs OK (avg 229ms); full registry (100+ URLs) not yet tested"}
{"sf_id": "SF-008", "file": "docs/arreglos chat/fase-3/q1-sources/Q1.1-BIBLIOTECA-OFICIAL-v0-REPORT.md", "line": 93, "original": "| G4 | Link checker smoke test | PASS | 3/3 URLs OK, JSONL generated |", "problem": "Gate G4 is marked PASS but the forensic audit explicitly found this MISLEADING: dry-run works but live test crashes at URL 12/18 (F-01). Reporting PASS with evidence \"3/3 URLs OK\" when the tool crashes during extended use is materially deceptive. Even post-fix, the evidence column should disclose the limited scope.", "severity": "CRITICAL", "replacement": "| G4 | Link checker smoke test | PASS (limited) | 3/3 sampled URLs OK in smoke mode; full-registry live test not completed (crash bug fixed post-audit) |"}
{"sf_id": "SF-009", "file": "docs/arreglos chat/fase-3/q1-sources/Q1-REPORT.md", "line": 11, "original": "a comprehensive catalog of official Spanish government sources at all three administrative levels (central, regional, local)", "problem": "\"comprehensive catalog\" overstates coverage. At the AGE level, 25 sources out of an unknown but much larger total (SIA alone catalogs thousands of procedures). At CCAA level, 19 entries = one per community (just the sede URL, not the procedure catalog). At local level, 20/8,131 municipalities = 0.25% coverage. This is a seed catalog, not a comprehensive one.", "severity": "HIGH", "replacement": "a seed catalog of official Spanish government sources spanning all three administrative levels (central: 25 sources, regional: 19 community profiles, local: 20 of 8,131 municipalities)"}
{"sf_id": "SF-010", "file": "docs/arreglos chat/fase-3/q1-sources/Q1-REPORT.md", "line": 14, "original": "all critical sedes electronicas", "problem": "\"all critical\" is an unverifiable claim. What defines \"critical\"? No criteria are given. The 25 AGE sources include some major sedes (SEPE, Seg Social, AEAT) but exclude others that could be critical to vulnerable users (e.g., specific Ministry sedes for housing, healthcare cards at CCAA level). Without a defined universe, \"all\" is unsubstantiated.", "severity": "MEDIUM", "replacement": "key sedes electronicas (SEPE, Seg Social, AEAT, Extranjeria, and others)"}
{"sf_id": "SF-011", "file": "docs/arreglos chat/fase-3/q1-sources/Q1-REPORT.md", "line": 16, "original": "20 municipal sedes verified (Tier 1 top cities by population)", "problem": "\"verified\" without qualification implies HTTP verification (URLs confirmed reachable). The Q1 report itself states \"No actual HTTP validation of URLs\" (line 140, Gap #1). The verification was research-level: URLs were documented from government directories, not HTTP HEAD-checked. The forensic audit also found the link checker crashed before completing verification.", "severity": "CRITICAL", "replacement": "20 municipal sedes documented with URLs from official directories (Tier 1 top cities by population); HTTP verification deferred to Q2"}
{"sf_id": "SF-012", "file": "docs/arreglos chat/fase-3/q1-sources/Q1-REPORT.md", "line": 58, "original": "| Tier 1 (P0) | Top 20 cities | Manual curation, verified URLs | 20 cities |", "problem": "\"verified URLs\" in the method column implies URLs were validated as working. Same issue as SF-011 -- no HTTP verification was performed in Q1. The backlog (Q1-CARRY-01 through Q1-CARRY-03) explicitly lists \"Verify all URLs are live\" as open items for Q2.", "severity": "HIGH", "replacement": "| Tier 1 (P0) | Top 20 cities | Manual curation, research-documented URLs | 20 cities |"}
{"sf_id": "SF-013", "file": "docs/arreglos chat/fase-3/q1-sources/Q1-REPORT.md", "line": 129, "original": "| G4 | Allowlist complete (3 tiers + blocklist) | **PASS** |", "problem": "\"Allowlist complete\" is misleading. The forensic audit (F-03) found 14 allowlist coverage gaps: 5 registry URLs with www.*/web.* subdomains not covered, and 9 local_seed URLs using .org TLD that would be REJECTED by the allowlist-first policy. A PASS for \"complete\" when 14 registered URLs would fail the policy is a contradiction.", "severity": "HIGH", "replacement": "| G4 | Allowlist covers 3 tiers + blocklist (14 coverage gaps identified in audit) | **PASS with caveats** |"}
{"sf_id": "SF-014", "file": "docs/arreglos chat/fase-3/q1-sources/Q1-REPORT.md", "line": 133, "original": "All 4 abort conditions cleared.", "problem": "\"All 4 abort conditions cleared\" is stated without caveat, but the forensic audit flagged A3 (Allowlist not enforceable) as a \"CONCERN\" due to the 14 coverage gaps. While not technically triggered (the allowlist exists and is enforceable in principle), calling it unconditionally \"cleared\" elides the concern.", "severity": "MEDIUM", "replacement": "All 4 abort conditions cleared (A3 flagged as concern in forensic audit due to 14 allowlist coverage gaps)."}
{"sf_id": "SF-015", "file": "docs/arreglos chat/fase-3/q1-sources/Q1-REPORT.md", "line": 4, "original": "Status: COMPLETE (research-only, no code changes)", "problem": "While the parenthetical is helpful, \"COMPLETE\" as a status for a quarter that explicitly defers all validation, all HTTP checks, all API testing, and all real-world verification to Q2 is semantically inflated. The research documentation is complete; the work of establishing a validated source-of-truth layer is not.", "severity": "MEDIUM", "replacement": "Status: RESEARCH COMPLETE (documentation only -- validation, HTTP checks, and API testing deferred to Q2)"}
{"sf_id": "SF-016", "file": "docs/arreglos chat/fase-3/q1-sources/Q1.1-BIBLIOTECA-OFICIAL-v0-REPORT.md", "line": 4, "original": "**Status:** COMPLETE", "problem": "Q1.1 status is \"COMPLETE\" without the same \"research-only\" caveat that Q1-REPORT.md uses. Q1.1 did produce code (scripts, schemas), but the link checker had a crash bug, the allowlist had 14 gaps, and the test suite had no negative cases -- all discovered by the forensic audit and reportedly fixed. If fixes were applied, this is accurate; if not, it is misleading. Either way, it lacks caveats present in Q1.", "severity": "MEDIUM", "replacement": "**Status:** COMPLETE (post-audit fixes applied; see FORENSIC-AUDIT-REPORT for 10 resolved findings)"}
{"sf_id": "SF-017", "file": "docs/arreglos chat/fase-3/q1-sources/evidence/gates.md", "line": 11, "original": "Local coverage strategy includes at least 20 cities with verified sede electronica URLs | **PASS** (20 cities, all verified)", "problem": "\"all verified\" in a gate evidence row implies mechanical verification. The Q1 scope explicitly excluded HTTP validation. The word \"verified\" here means \"researched and documented\" but reads as \"confirmed working via automated check.\" The backlog item Q1-CARRY-03 explicitly says this verification is pending.", "severity": "HIGH", "replacement": "Local coverage strategy includes at least 20 cities with documented sede electronica URLs | **PASS** (20 cities, all URLs research-documented; HTTP verification deferred to Q2)"}
{"sf_id": "SF-018", "file": "docs/arreglos chat/fase-3/q1-sources/evidence/gates.md", "line": 45, "original": "Tier 1 cities: 20 (all with verified sede electronica URLs)", "problem": "Same \"verified\" inflation as SF-017. Repeated in the gate evidence detail section. Creates reinforcement effect -- the more times \"verified\" appears, the more readers assume HTTP verification happened.", "severity": "HIGH", "replacement": "Tier 1 cities: 20 (all with research-documented sede electronica URLs; HTTP verification pending Q2)"}
{"sf_id": "SF-019", "file": "docs/arreglos chat/fase-3/q1-sources/evidence/gates.md", "line": 88, "original": "| Gates passed | 6/6 |", "problem": "\"6/6\" in a summary table without caveat conceals that G4 was found MISLEADING by the forensic audit. The fraction \"6/6\" implies perfect, clean passes. Should note the G4 caveat.", "severity": "HIGH", "replacement": "| Gates passed | 6/6 (G4 limited to smoke/dry-run; see forensic audit) |"}
{"sf_id": "SF-020", "file": "docs/arreglos chat/fase-3/q1-sources/evidence/gates.md", "line": 92, "original": "| Municipal URLs verified | 20 |", "problem": "\"verified\" in the summary metrics table. Same issue as SF-017/SF-018 -- no HTTP verification was performed. This is the third repetition of the misleading \"verified\" claim in the same file.", "severity": "HIGH", "replacement": "| Municipal URLs documented | 20 (HTTP verification pending Q2) |"}
{"sf_id": "SF-021", "file": "docs/arreglos chat/fase-3/q1-sources/evidence/gates.md", "line": 108, "original": "| **G4: Link checker works** | `link_check.py` runs smoke test and generates JSONL output | **PASS** (3/3 URLs OK) |", "problem": "Q1.1 gate G4 says \"Link checker works\" and \"PASS (3/3 URLs OK)\" but the forensic audit found the link checker crashes at URL 12/18. \"Works\" is overstated when the tool has a crash bug on real data. The 3/3 denominator is the smoke test limit, not total URLs.", "severity": "CRITICAL", "replacement": "| **G4: Link checker runs** | `link_check.py` runs smoke test (3 URLs) and generates JSONL output | **PASS (smoke only)** — full-registry test had crash bug (fixed post-audit) |"}
{"sf_id": "SF-022", "file": "docs/arreglos chat/fase-3/q1-sources/evidence/Q1.1-FORENSIC-AUDIT-REPORT.md", "line": 165, "original": "All 10 audit findings have been resolved. Gates re-verified: 5/5 PASS.", "problem": "\"All 10 audit findings have been resolved\" in the same document that found them is a temporal confusion risk. Were the fixes applied to the actual artifact files, or only documented here as \"FIXED\"? The forensic audit report says fixes were applied, but the Q1.1 report (which it audited) still contains some of the original misleading language (e.g., \"3/3 PASS\" in gates, \"all passing\" in bullet 5). If fixes were applied to code but NOT propagated to the Q1.1 report prose, then \"resolved\" is only partially true.", "severity": "HIGH", "replacement": "All 10 audit findings have been addressed. Code/data fixes applied; report prose corrections should be verified independently. Gates re-verified post-fix: 5/5 PASS."}
{"sf_id": "SF-023", "file": "docs/arreglos chat/fase-3/q1-sources/evidence/Q1.1-FORENSIC-AUDIT-REPORT.md", "line": 14, "original": "Hallucination Risk Score: 12/100 (LOW)", "problem": "A numeric hallucination risk score (12/100) with a parenthetical severity label (LOW) implies a calibrated, standardized metric. There is no disclosed methodology for this score. What does 12 mean? How was it calculated? Without a rubric, this is a subjective impression dressed up as a quantitative measurement, which is itself a form of semantic inflation.", "severity": "MEDIUM", "replacement": "Hallucination Risk Assessment: LOW — Core data claims (counts, file existence, schemas) are accurate; discrepancies found in secondary metrics and one crash bug (see findings below). Note: this is a qualitative assessment, not a calibrated metric."}
{"sf_id": "SF-024", "file": "docs/arreglos chat/fase-3/q1-sources/evidence/Q1.1-FORENSIC-AUDIT-REPORT.md", "line": 12, "original": "the vast majority are VERIFIED", "problem": "\"the vast majority\" is vague and unquantifiable. How many of 325 claims were actually individually checked? The claims ledger shows ~30 key claims verified. Were the other 295 claims checked or assumed correct? If only the \"most critical\" were verified (as the report says on line 70), then \"vast majority VERIFIED\" overstates the verification coverage.", "severity": "MEDIUM", "replacement": "the ~30 most critical claims were individually verified (counts, file existence, schema validation, gates); remaining claims were not individually audited"}
{"sf_id": "SF-025", "file": "docs/arreglos chat/fase-3/q1-sources/evidence/Q1.1-FORENSIC-AUDIT-REPORT.md", "line": 26, "original": "Core claims (44 sources, 25 AGE, 19 CCAA, 20 local, 6/6 gates) are all VERIFIED", "problem": "\"6/6 gates\" listed as \"all VERIFIED\" contradicts the same report's own finding on line 103 where G4 is marked \"PARTIAL\" / \"MISLEADING.\" Internal inconsistency: the abort condition table claims 6/6 VERIFIED, but the gate claims table says G4 is not fully verified.", "severity": "HIGH", "replacement": "Core claims (44 sources, 25 AGE, 19 CCAA, 20 local) are VERIFIED; gates are 5/6 clean PASS + G4 partial (see F-01)"}
{"sf_id": "SF-026", "file": "docs/arreglos chat/fase-3/q1-sources/evidence/references.md", "line": 30, "original": "## Sedes Electronicas (Verified)", "problem": "Section header \"Verified\" implies HTTP verification of all sedes electronicas listed. The section just cross-references age.md, ccaa.md, and local.md -- none of which were HTTP-verified in Q1. The header creates a false impression of validation status.", "severity": "HIGH", "replacement": "## Sedes Electronicas (Research-Documented)"}
{"sf_id": "SF-027", "file": "docs/arreglos chat/fase-3/q1-sources/evidence/references.md", "line": 39, "original": "See `source-registry/local.md` for complete list with verified URLs.", "problem": "\"complete list with verified URLs\" -- \"complete\" is misleading because it is 20 of 8,131 municipalities (0.25%). \"verified URLs\" has the same HTTP verification problem. This sentence doubly inflates: both completeness and verification status.", "severity": "HIGH", "replacement": "See `source-registry/local.md` for the Tier 1 seed list (20 cities) with research-documented URLs."}
{"sf_id": "SF-028", "file": "docs/arreglos chat/fase-3/q1-sources/evidence/references.md", "line": 63, "original": "| Municipal URLs verified | 20/20 (Tier 1) | This research |", "problem": "\"20/20 (Tier 1)\" as a fraction creates the impression of 100% completion. The actual denominator is 8,131 municipalities, making the true coverage 0.25%. The fraction hides the scope limitation. Additionally, \"verified\" remains unqualified.", "severity": "HIGH", "replacement": "| Municipal URLs documented | 20/8,131 (Tier 1 seed cities; 0.25% of all municipalities) | This research |"}
{"sf_id": "SF-029", "file": "docs/arreglos chat/fase-3/q1-sources/evidence/references.md", "line": 33, "original": "See `source-registry/age.md` for complete list with URLs.", "problem": "\"complete list\" implies all AGE sources are cataloged. Spain's SIA catalog contains thousands of procedures; 25 sources is a curated selection, not a complete enumeration.", "severity": "MEDIUM", "replacement": "See `source-registry/age.md` for the curated list of 25 AGE sources with URLs."}
{"sf_id": "SF-030", "file": "docs/arreglos chat/fase-3/q1-sources/source-registry/ccaa.md", "line": 4, "original": "Status: Complete — all 19 communities/cities profiled with verified URLs", "problem": "\"Complete\" and \"verified URLs\" in the CCAA registry header. Each of the 19 entries has a sede URL, but \"verified\" again implies HTTP checking that was not done. \"Complete\" is arguably accurate for 19/19 communities, but the profiles are one-entry-per-community, not comprehensive procedure catalogs -- the forensic audit notes that CCAA page layouts were not analyzed for extraction (Gap G4).", "severity": "HIGH", "replacement": "Status: All 19 communities/cities profiled with research-documented sede URLs (HTTP verification and extraction compatibility deferred to Q2)"}
{"sf_id": "SF-031", "file": "docs/arreglos chat/fase-3/q1-sources/source-registry/local.md", "line": 6, "original": "20 largest cities by population, manually curated with verified sede electronica URLs and procedure catalogs", "problem": "\"verified sede electronica URLs\" -- same HTTP verification issue. \"procedure catalogs\" implies actual catalog content was indexed; only the catalog page URL was documented, not the catalog contents.", "severity": "HIGH", "replacement": "20 largest cities by population, manually curated with research-documented sede electronica URLs and links to procedure catalog pages"}
{"sf_id": "SF-032", "file": "docs/arreglos chat/fase-3/q1-sources/source-registry/local.md", "line": 18, "original": "Each has been verified with real sede electronica URLs.", "problem": "\"verified with real sede electronica URLs\" conflates two things: (1) the URLs are real (from official directories) and (2) they have been verified (HTTP-checked). Only (1) is true. The sentence structure makes it sound like active verification was performed.", "severity": "HIGH", "replacement": "Each has a documented sede electronica URL sourced from official government directories. (HTTP reachability not yet confirmed -- see Q2 backlog.)"}
{"sf_id": "SF-033", "file": "docs/arreglos chat/fase-3/q1-sources/Q1.1-BIBLIOTECA-OFICIAL-v0-REPORT.md", "line": 74, "original": "| `tests/unit/test_validators.py` | 3 | 3/3 PASS |", "problem": "\"3/3 PASS\" may be stale if the forensic audit's AUDIT-04 fix (adding 2 negative tests) was applied -- the total should now be 5/5. If the fix was applied, this row is stale (temporal confusion). If not, the forensic audit claim of \"RESOLVED\" is false. Either way, one document is wrong.", "severity": "MEDIUM", "replacement": "| `tests/unit/test_validators.py` | 5 | 5/5 PASS (updated post-audit: 3 original + 2 negative tests) |"}
{"sf_id": "SF-034", "file": "docs/arreglos chat/fase-3/q1-sources/Q1.1-BIBLIOTECA-OFICIAL-v0-REPORT.md", "line": 5, "original": "Convert Q1 research markdown into machine-readable, validated artifacts for Clara's RAG pipeline.", "problem": "\"validated artifacts\" in the goal statement sets an expectation of validation that is only partially met. The artifacts are schema-validated (structural), not content-validated (HTTP liveness, accuracy) or operationally validated (used in a real RAG pipeline). Clara's RAG pipeline does not yet exist (RAG_ENABLED=false per CLAUDE.md).", "severity": "MEDIUM", "replacement": "Convert Q1 research markdown into machine-readable, schema-validated artifacts for Clara's future RAG pipeline."}
{"sf_id": "SF-035", "file": "docs/arreglos chat/fase-3/q1-sources/Q1-REPORT.md", "line": 42, "original": "19/19 CCAA profiles", "problem": "\"19/19\" as a metric conceals what each \"profile\" actually contains. Each CCAA profile is essentially one entry with a sede URL, priority level, and co-official language notation. A reader might interpret \"profile\" as a comprehensive analysis of each community's procedure ecosystem. The actual depth is quite shallow -- the forensic audit notes that CCAA page layouts were not analyzed for extraction (Gap G4).", "severity": "MEDIUM", "replacement": "19/19 CCAA entries (sede URL + priority + language notes per community; detailed procedure analysis deferred to Q2)"}
{"sf_id": "SF-036", "file": "docs/arreglos chat/fase-3/q1-sources/evidence/Q1.1-FORENSIC-AUDIT-REPORT.md", "line": 213, "original": "pytest tests/unit/test_validators.py -v                # 3/3 PASS", "problem": "The Commands Used section shows \"3/3 PASS\" but if AUDIT-04 was resolved (adding 2 negative tests), the actual result should be 5/5 PASS. This is either stale (command was run before the fix) or the fix was not actually applied. Either way, the comment is inconsistent with the \"RESOLVED\" status of AUDIT-04 in the same document.", "severity": "MEDIUM", "replacement": "pytest tests/unit/test_validators.py -v                # 5/5 PASS (post-audit, includes 2 negative tests)"}
